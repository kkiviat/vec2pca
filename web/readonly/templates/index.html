{% extends "base_template.html" %}
{% block body %}

<div class="container grey lighten-5">
		<div class="container section about">
			<h1 class="center-align">About Vec2PCA</h1>
			<h5>Visualizing topics in text with Word2Vec and Principal Component Analysis</h5>
			<div class="divider"></div>
			<p>
				This tool aims to reveal the important underlying topics in a corpus of text, like a book or the comments of an online community.
			</p>
			<p>
				Word2Vec is a shallow neural algorithm (originally released in a paper by Google) for mapping text onto a vector space. Each word becomes a vector and the vectors near each other have similar meanings and usages. A vector corresponding to a word has semantic meaning, and preserves things like analogies:
			</p>
			<div class="card">
				<div class="card-image">
					<img class="responsive-img" src="https://www.tensorflow.org/versions/r0.7/images/linear-relationships.png">
					<div class="card-content right-align">
						<a href="https://tensorflow.org" style="font-size: 1.1rem">Via tensorflow.org</a>
					</div>
				</div>
			</div>
			<p>
				Word2Vec is very useful, but it's somewhat difficult for humans to interpret. It produces an unwieldy high-dimensional space, and directions in that space can have unclear meaning. A solution is PCA (Principal Component Analysis). PCA is a technique for reducing the dimensionality of data. It essentially rotates the data to get a better view, or rather, so that the variance in the data is best explained in as few dimensions as possible. Here's what it can look like when reducing from three dimensions to two:
			</p>
			<div class="card">
				<div class="card-image">
					<img class="responsive-img" src="http://cnx.org/resources/269e78e2506cdbdc7f45ca17bb7d83491ce0c644/pca.jpg">
					<div class="card-content right-align">
						<a href="https://cnx.org" style="font-size: 1.1rem">Via cnx.org</a>
					</div>
				</div>
			</div>
			<p>
				I combined these two techniques into something I've named Vec2PCA (keeping with the naming tradition for derivatives and variants of Word2Vec). Vec2PCA pulls some striking patterns out of text. There usually aren't good English words for the category that a principal component represents, but it will probably be a concept you know well. My friends and I found interpreting the results to be a fascinating experience, but it's previously been restricted to my terminal window. To fix that, I made a viewer and ran Vec2PCA on a few interesting corpuses. I used <a href="https://cloud.google.com/compute/">Google Compute Engine</a> to churn through gigabytes of text, and the whole thing is written in <a href="https://python.org">Python</a> with some great libraries: <a href="https://radimrehurek.com/gensim/">Gensim</a>, <a href="https://scikit-learn.org">Scikit-Learn</a>, <a href="http://flask.pocoo.org/">Flask</a>, <a href="http://www.celeryproject.org/">Celery</a> with <a href="http://redis.io/">Redis</a>, and <a href="http://pandas.pydata.org/">Pandas</a>. Take a look!
			</p>
			<ul class="collection">
			
			<li>
		  		<div class="collection-item">
		    		<h5>Reddit</h5>
		    		<p>All reddit comments in May 2015. The rest of the dataset is very large (1TB) and is forthcoming. Retrieved from Google BigQuery.</p>
		  		</div>
		  	</li>
		  	<li>
				<a href="results/hn_components.csv" class="collection-item">
					<h5>Hacker News</h5>
				    <p>All HN comments, also retrieved from BigQuery.</p>
				</a>
		  	</li>
		  	<li>
				<div class="collection-item">
		    		<h5>Wikipedia</h5>
		    		<p>Full dump as of May 2016</p>
		  		</div>
			</li>
			<li>
				<a href="results/lw_components.csv" class="collection-item">
		    		<h5>Lesswrong</h5>
		    		<p>Comments as of around January 2016.</p>
		  		</a>
			</li>
			<li>
				<div class="collection-item">
		    		<h5>Project Gutenberg</h5>
		    		<p>Lots and lots of books.</p>
		  		</div>
			</li>
		</ul>
		</div>
</div>

{% endblock %}